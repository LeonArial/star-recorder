"""
è¯­éŸ³è¯†åˆ«APIæœåŠ¡å™¨
æä¾›RESTful APIæ¥å£è¿›è¡ŒéŸ³é¢‘è½¬å½•
æ”¯æŒä¸‰ç§è¯†åˆ«ç»“æœå¯¹æ¯”ï¼šParaformer + SenseVoice + LLMæ™ºèƒ½åˆå¹¶
"""
import os
import tempfile
import wave
import json
import threading
import numpy as np
from flask import Flask, request, jsonify
from flask_socketio import SocketIO, emit
from flask_cors import CORS
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import soundfile as sf
import librosa
import requests
import re
import traceback
import emoji

app = Flask(__name__)
app.config['SECRET_KEY'] = 'asr-api-server'
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# SocketIO é…ç½®ï¼ˆä¼˜åŒ–é•¿æ—¶é—´å½•éŸ³ç¨³å®šæ€§ï¼‰
socketio = SocketIO(
    app, 
    cors_allowed_origins="*", 
    async_mode='threading',
    async_handlers=False,
    # å¢åŠ  ping è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤20ç§’å¤ªçŸ­ï¼Œé•¿æ—¶é—´å½•éŸ³å¯èƒ½è¶…æ—¶ï¼‰
    ping_timeout=120,  # 120ç§’è¶…æ—¶
    ping_interval=30,  # æ¯30ç§’å‘é€ä¸€æ¬¡ping
    # å¢åŠ æœ€å¤§ç¼“å†²åŒºå¤§å°ï¼ˆæ”¯æŒæ›´å¤§çš„éŸ³é¢‘æ•°æ®å¸§ï¼‰
    max_http_buffer_size=10 * 1024 * 1024,  # 10MB
)

# å…¨å±€æ¨¡å‹å®ä¾‹
asr_model = None
punc_realtime_model = None  # å®æ—¶æ ‡ç‚¹æ¨¡å‹
vad_model = None  # VADè¯­éŸ³ç«¯ç‚¹æ£€æµ‹æ¨¡å‹
sensevoice_model = None

# å…¨å±€æ¨¡å‹æ¨ç†é”ï¼ˆthreading æ¨¡å¼ä¸‹é¿å…å¹¶å‘æ¨ç†å¯¼è‡´ç¼“å­˜/å†…éƒ¨çŠ¶æ€ç«äº‰ï¼‰
asr_model_lock = threading.Lock()
punc_model_lock = threading.Lock()
vad_model_lock = threading.Lock()
sensevoice_model_lock = threading.Lock()

# LLMé…ç½®
LLM_API_URL = "http://10.8.75.207:9997/v1/chat/completions"
LLM_API_KEY = "sk-dmowsenrtifmlnpmlhaatxgkxnhbmusjfzgnofvlhtblslwa"
LLM_MODEL = "qwen3:8b"

# æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'ogg', 'flac', 'm4a', 'aac', 'wma', 'webm'}

# æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆDockeræŒ‚è½½æˆ–æœ¬åœ°ç›®å½•ï¼‰
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„ models_cache
MODELS_CACHE_DIR = os.environ.get('MODELSCOPE_CACHE', 
    os.path.join(os.path.dirname(__file__), 'models_cache'))
HF_CACHE_DIR = os.environ.get('HF_HOME',
    os.path.join(os.path.dirname(__file__), 'hf_cache'))

# å­˜å‚¨å®æ—¶å½•éŸ³ä¼šè¯
active_sessions = {}
active_sessions_lock = threading.Lock()

def init_models():
    """åˆå§‹åŒ– ASRã€æ ‡ç‚¹ã€VADä¸å¤æ£€æ¨¡å‹
    
    æ¨¡å‹ç¼“å­˜ç­–ç•¥ï¼š
    - ä¼˜å…ˆä» MODELSCOPE_CACHE ç›®å½•åŠ è½½å·²æœ‰æ¨¡å‹
    - å¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½åˆ°ç¼“å­˜ç›®å½•
    - Dockerè¿è¡Œæ—¶é€šè¿‡æŒ‚è½½å·æŒä¹…åŒ–æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
    """
    global asr_model, punc_realtime_model, vad_model, sensevoice_model
    
    if asr_model is None:
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # è®¾ç½®æ¨¡å‹ç¼“å­˜ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿FunASRä½¿ç”¨æ­£ç¡®çš„ç¼“å­˜è·¯å¾„ï¼‰
        os.environ['MODELSCOPE_CACHE'] = MODELS_CACHE_DIR
        os.environ['HF_HOME'] = HF_CACHE_DIR
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(MODELS_CACHE_DIR, exist_ok=True)
        os.makedirs(HF_CACHE_DIR, exist_ok=True)
        
        print(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {MODELS_CACHE_DIR}")
        print(f"ğŸ“ HuggingFaceç¼“å­˜ç›®å½•: {HF_CACHE_DIR}")
        
        # æ£€æµ‹è®¾å¤‡ï¼ˆCUDA GPU > Apple MPS > CPUï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                # NVIDIA GPUï¼ˆLinux/Windows æœåŠ¡å™¨ï¼‰
                device = "cuda:0"
                print(f"âœ… æ£€æµ‹åˆ° CUDA GPU: {torch.cuda.get_device_name(0)}")
            elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
                # Apple Silicon MPSï¼ˆM1/M2/M3/M4 Macï¼‰
                device = "mps"
                print("âœ… æ£€æµ‹åˆ° Apple Siliconï¼Œä½¿ç”¨ MPS åŠ é€Ÿ")
            else:
                device = "cpu"
                print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")
        except Exception as e:
            device = "cpu"
            print(f"âš ï¸ è®¾å¤‡æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ CPU æ¨¡å¼: {e}")
        
        # FunASR æ¨¡å‹ååˆ°å®é™…ç›®å½•åçš„æ˜ å°„
        MODEL_DIR_MAP = {
            "paraformer-zh-streaming": "speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online",
            "iic/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727": "punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727",
            "fsmn-vad": "speech_fsmn_vad_zh-cn-16k-common-pytorch",
            "iic/SenseVoiceSmall": "SenseVoiceSmall",
        }
        
        def get_model_path(model_name):
            """è·å–æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼Œå¦‚æœå·²ç¼“å­˜åˆ™è¿”å›æœ¬åœ°è·¯å¾„ï¼Œå¦åˆ™è¿”å›æ¨¡å‹åï¼ˆè§¦å‘ä¸‹è½½ï¼‰"""
            actual_name = MODEL_DIR_MAP.get(model_name, model_name.split('/')[-1])
            local_path = os.path.join(MODELS_CACHE_DIR, 'models', 'iic', actual_name)
            if os.path.exists(local_path):
                return local_path, True  # è¿”å›æœ¬åœ°è·¯å¾„
            return model_name, False  # è¿”å›æ¨¡å‹åè§¦å‘ä¸‹è½½
        
        # åŠ è½½ä¸­æ–‡æµå¼ ASR æ¨¡å‹
        model_name = "paraformer-zh-streaming"
        model_path, is_cached = get_model_path(model_name)
        print(f"  - åŠ è½½ ASR æ¨¡å‹: {model_name} {'(å·²ç¼“å­˜)' if is_cached else '(é¦–æ¬¡ä¸‹è½½)'} (è®¾å¤‡: {device})")
        asr_model = AutoModel(
            model=model_path,
            device=device,
            disable_update=True,
        )
        
        # åŠ è½½å®æ—¶æ ‡ç‚¹æ¨¡å‹ï¼ˆæ”¯æŒæµå¼å¤„ç†ï¼Œå¸¦ç¼“å­˜ï¼‰
        model_name = "iic/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727"
        model_path, is_cached = get_model_path(model_name)
        print(f"  - åŠ è½½å®æ—¶æ ‡ç‚¹æ¨¡å‹: punc_realtime {'(å·²ç¼“å­˜)' if is_cached else '(é¦–æ¬¡ä¸‹è½½)'} (è®¾å¤‡: {device})")
        punc_realtime_model = AutoModel(
            model=model_path,
            device=device,
            disable_update=True,
        )
        
        # åŠ è½½VADè¯­éŸ³ç«¯ç‚¹æ£€æµ‹æ¨¡å‹ï¼ˆå®æ—¶ï¼‰
        model_name = "fsmn-vad"
        model_path, is_cached = get_model_path(model_name)
        print(f"  - åŠ è½½VADæ¨¡å‹: {model_name} {'(å·²ç¼“å­˜)' if is_cached else '(é¦–æ¬¡ä¸‹è½½)'} (è®¾å¤‡: {device})")
        vad_model = AutoModel(
            model=model_path,
            device=device,
            disable_update=True,
        )
        
        # SenseVoice å¤æ£€æ¨¡å‹ï¼ˆé…ç½®VADï¼‰
        model_name = "iic/SenseVoiceSmall"
        model_path, is_cached = get_model_path(model_name)
        vad_path, _ = get_model_path("fsmn-vad")  # VAD æ¨¡å‹è·¯å¾„
        print(f"  - åŠ è½½å¤æ£€æ¨¡å‹: SenseVoiceSmall {'(å·²ç¼“å­˜)' if is_cached else '(é¦–æ¬¡ä¸‹è½½)'} (è®¾å¤‡: {device})")
        sensevoice_model = AutoModel(
            model=model_path,
            vad_model=vad_path,
            vad_kwargs={"max_single_segment_time": 30000},
            device=device,
            disable_update=True,
            use_itn=True,
        )
        
        print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")


def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def _clean_sensevoice_text(text):
    """æ¸…ç† SenseVoice è¾“å‡ºä¸­çš„è™šå‡æ–‡æœ¬
    
    SenseVoice æœ‰æ—¶ä¼šè¾“å‡ºå®é™…è¯­éŸ³ä¸­ä¸å­˜åœ¨çš„å¡«å……è¯ï¼Œå¦‚ Yeah./Okay./Oh./Hmm. ç­‰
    """
    if not text:
        return text
    
    # éœ€è¦ç§»é™¤çš„è™šå‡æ–‡æœ¬æ¨¡å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    fake_patterns = [
        r'\bYeah\.?\s*',
        r'\bOkay\.?\s*',
        r'\bOK\.?\s*',
        r'\bOh\.?\s*',
        r'\bHmm\.?\s*',
        r'\bUh\.?\s*',
        r'\bUm\.?\s*',
        r'\bAh\.?\s*',
        r'\bEh\.?\s*',
        r'\bWell\.?\s*',
    ]
    
    cleaned = text
    for pattern in fake_patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned


def _run_sensevoice(audio_path):
    """ä½¿ç”¨SenseVoiceè¿›è¡Œå®Œæ•´éŸ³é¢‘è¯†åˆ«ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰"""
    try:
        with sensevoice_model_lock:
            result = sensevoice_model.generate(
                input=audio_path,
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=True,
                merge_length_s=15,  # åˆå¹¶åçš„éŸ³é¢‘ç‰‡æ®µé•¿åº¦
            )
        
        if result and len(result) > 0:
            raw_text = result[0].get("text", "")
            # ä½¿ç”¨å®˜æ–¹çš„å¯Œæ–‡æœ¬åå¤„ç†å‡½æ•°æ¸…ç†ç‰¹æ®Šæ ‡è®°
            clean_text = rich_transcription_postprocess(raw_text)
            # å»é™¤emoji
            clean_text = emoji.replace_emoji(clean_text, replace='')
            # å»é™¤è™šå‡å¡«å……è¯
            clean_text = _clean_sensevoice_text(clean_text)
            return clean_text
        
        return ""
        
    except Exception as e:
        raise Exception(f"SenseVoiceè¯†åˆ«å¤±è´¥: {str(e)}")


def _run_sensevoice_with_timestamps(audio_path):
    """ä½¿ç”¨ç‹¬ç«‹VADæ¨¡å‹è·å–è¯­éŸ³æ®µæ—¶é—´æˆ³ï¼Œå†ç”¨SenseVoiceè¯†åˆ«æ¯æ®µ
    
    Returns:
        tuple: (full_text, segments)
            - full_text: å®Œæ•´æ–‡æœ¬
            - segments: å¥çº§æ—¶é—´æˆ³åˆ—è¡¨ [{'text': 'å¥å­', 'start_ms': 0, 'end_ms': 1000}, ...]
    """
    try:
        # å…ˆä½¿ç”¨ç‹¬ç«‹VADæ¨¡å‹æ£€æµ‹è¯­éŸ³æ®µ
        print("ğŸ” VADæ£€æµ‹è¯­éŸ³æ®µ...")
        with vad_model_lock:
            vad_result = vad_model.generate(
                input=audio_path,
                cache={},
            )
        
        # è§£æVADç»“æœï¼Œæ ¼å¼ä¸º [[start1, end1], [start2, end2], ...]
        vad_segments = []
        if vad_result and len(vad_result) > 0:
            vad_data = vad_result[0].get("value", [])
            if vad_data:
                vad_segments = vad_data
        
        print(f"  ğŸ“Š VADæ£€æµ‹åˆ° {len(vad_segments)} ä¸ªè¯­éŸ³æ®µ")
        
        # å¦‚æœVADæ²¡æœ‰æ£€æµ‹åˆ°åˆ†æ®µï¼Œä½¿ç”¨SenseVoiceæ•´ä½“è¯†åˆ«
        if not vad_segments:
            print("  âš ï¸ VADæœªæ£€æµ‹åˆ°åˆ†æ®µï¼Œä½¿ç”¨æ•´ä½“è¯†åˆ«")
            text = _run_sensevoice(audio_path)
            return text, [{'text': text, 'start_ms': 0, 'end_ms': 0}] if text else (text, [])
        
        # è¯»å–éŸ³é¢‘æ•°æ®
        audio_data, sr = librosa.load(audio_path, sr=16000, mono=True)
        
        segments = []
        
        # å¯¹æ¯ä¸ªVADæ®µè¿›è¡Œè¯†åˆ«
        for i, (start_ms, end_ms) in enumerate(vad_segments):
            # è½¬æ¢ä¸ºé‡‡æ ·ç‚¹
            start_sample = int(start_ms * sr / 1000)
            end_sample = int(end_ms * sr / 1000)
            
            # æå–éŸ³é¢‘æ®µ
            segment_audio = audio_data[start_sample:end_sample]
            
            if len(segment_audio) < sr * 0.1:  # å°‘äº 0.1 ç§’è·³è¿‡
                continue
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ç”¨äºè¯†åˆ«
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
            temp_path = temp_file.name
            temp_file.close()
            sf.write(temp_path, segment_audio, sr)
            
            try:
                # è¯†åˆ«è¯¥æ®µ
                with sensevoice_model_lock:
                    result = sensevoice_model.generate(
                        input=temp_path,
                        cache={},
                        language="auto",
                        use_itn=True,
                    )
                
                if result and len(result) > 0:
                    raw_text = result[0].get("text", "")
                    clean_text = rich_transcription_postprocess(raw_text)
                    clean_text = emoji.replace_emoji(clean_text, replace='')
                    clean_text = _clean_sensevoice_text(clean_text)
                    
                    if clean_text.strip():
                        segments.append({
                            'text': clean_text,
                            'start_ms': int(start_ms),
                            'end_ms': int(end_ms)
                        })
                        print(f"  âœ… æ®µ{i+1}: {start_ms/1000:.1f}s-{end_ms/1000:.1f}s: {clean_text[:30]}...")
            finally:
                os.remove(temp_path)
        
        full_text = ''.join([seg['text'] for seg in segments])
        print(f"âœ… è¯†åˆ«å®Œæˆ: {len(full_text)}å­—, {len(segments)}æ®µ")
        return full_text, segments
        
    except Exception as e:
        print(f"âš ï¸ SenseVoiceæ—¶é—´æˆ³è¯†åˆ«å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return "", []


def _run_sensevoice_array(audio_array, sample_rate):
    """ä½¿ç”¨SenseVoiceè¿›è¡Œå®Œæ•´éŸ³é¢‘è¯†åˆ«ï¼ˆnumpyæ•°ç»„ï¼‰"""
    try:
        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        temp_path = temp_file.name
        temp_file.close()
        
        sf.write(temp_path, audio_array, sample_rate)
        
        with sensevoice_model_lock:
            result = sensevoice_model.generate(
                input=temp_path,
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=True,
                merge_length_s=15,  # åˆå¹¶åçš„éŸ³é¢‘ç‰‡æ®µé•¿åº¦
            )
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)
        
        if result and len(result) > 0:
            raw_text = result[0].get("text", "")
            clean_text = rich_transcription_postprocess(raw_text)
            # å»é™¤emoji
            clean_text = emoji.replace_emoji(clean_text, replace='')
            # å»é™¤è™šå‡å¡«å……è¯
            clean_text = _clean_sensevoice_text(clean_text)
            return clean_text
        
        return ""
        
    except Exception as e:
        raise Exception(f"SenseVoiceè¯†åˆ«å¤±è´¥: {str(e)}")


# ==================== å®æ—¶å½•éŸ³å¤„ç†ç±» ====================

class RealtimeASR:
    """å®æ—¶è¯­éŸ³è¯†åˆ«å¤„ç†å™¨
    
    ä¼˜åŒ–ç‰¹æ€§ï¼š
    - ä½¿ç”¨ fsmn-vad è¿›è¡Œå®æ—¶è¯­éŸ³ç«¯ç‚¹æ£€æµ‹
    - ä½¿ç”¨å®æ—¶æ ‡ç‚¹æ¨¡å‹è¿›è¡Œæµå¼æ ‡ç‚¹æ¢å¤
    - åŸºäº VAD ç»“æœæ™ºèƒ½åˆ†å¥ï¼Œæå‡è¯†åˆ«ä½“éªŒ
    """
    
    def __init__(self, session_id):
        self.session_id = session_id
        self.sample_rate = 16000
        self.lock = threading.Lock()
        self.is_finalizing = False
        
        # ASR ç›¸å…³é…ç½®
        self.audio_buffer = []  # ASR éŸ³é¢‘ç¼“å†²åŒº
        self.asr_cache = {}  # æµå¼ ASR è¯†åˆ«ç¼“å­˜
        self.chunk_size = [0, 10, 5]  # [0, 10, 5] è¡¨ç¤º 600ms å®æ—¶å‡ºå­—
        self.asr_chunk_stride = self.chunk_size[1] * 960  # 600ms = 9600 é‡‡æ ·ç‚¹
        
        # VAD ç›¸å…³é…ç½®
        self.vad_buffer = []  # VAD éŸ³é¢‘ç¼“å†²åŒº
        self.vad_cache = {}  # VAD æ£€æµ‹ç¼“å­˜
        self.vad_chunk_size = 200  # VAD æ£€æµ‹ç²’åº¦ 200ms
        self.vad_chunk_stride = int(self.vad_chunk_size * self.sample_rate / 1000)  # 3200 é‡‡æ ·ç‚¹
        self.is_speech_active = False  # å½“å‰æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³
        self.speech_start_time = 0  # è¯­éŸ³å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        self.total_audio_ms = 0  # å·²å¤„ç†çš„éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        
        # æ ‡ç‚¹ç›¸å…³é…ç½®
        self.punc_cache = {}  # å®æ—¶æ ‡ç‚¹ç¼“å­˜
        self.all_text = ""  # ç´¯ç§¯æ‰€æœ‰è¯†åˆ«æ–‡æœ¬ï¼ˆæ— æ ‡ç‚¹ï¼‰
        self.text_with_punc = ""  # å·²æ·»åŠ æ ‡ç‚¹çš„æ–‡æœ¬
        self.pending_text = ""  # ç­‰å¾…æ ‡ç‚¹çš„æ–‡æœ¬
        self.sentence_buffer = ""  # å½“å‰å¥å­ç¼“å†²åŒºï¼ˆVAD åˆ†å¥ç”¨ï¼‰
        
        # å®Œæ•´å½•éŸ³ç¼“å­˜ï¼ˆç”¨äº SenseVoice æœ€ç»ˆè¯†åˆ«ï¼‰
        self.full_audio = []
        
        # å®æ—¶æ—¶é—´æˆ³è·Ÿè¸ª
        self.asr_processed_ms = 0  # ASR å·²å¤„ç†çš„éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        self.segments = []  # å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ [{text, start_ms, end_ms}, ...]
        self.current_segment_start = 0  # å½“å‰ç‰‡æ®µèµ·å§‹æ—¶é—´
        
    def add_audio(self, audio_data):
        """æ·»åŠ éŸ³é¢‘æ•°æ®åˆ°ç¼“å†²åŒº"""
        try:
            # ç¡®ä¿æ•°æ®é•¿åº¦æ˜¯ 2 çš„å€æ•°ï¼ˆint16 = 2 bytesï¼‰
            if len(audio_data) % 2 != 0:
                audio_data = audio_data[:-1]
            
            if len(audio_data) == 0:
                return
            
            # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸º float32 numpy æ•°ç»„
            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            self.audio_buffer.extend(audio_np)
            self.vad_buffer.extend(audio_np)
            self.full_audio.extend(audio_np)  # ä¿å­˜å®Œæ•´éŸ³é¢‘ç”¨äº SenseVoice
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æ•°æ®å¤„ç†é”™è¯¯: {str(e)}")
    
    def _process_vad(self):
        """å¤„ç† VAD è¯­éŸ³ç«¯ç‚¹æ£€æµ‹
        
        è¿”å›å€¼ï¼š
        - None: æ²¡æœ‰æ£€æµ‹åˆ°ç«¯ç‚¹å˜åŒ–
        - {'type': 'start', 'time': ms}: æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
        - {'type': 'end', 'time': ms}: æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
        """
        if len(self.vad_buffer) < self.vad_chunk_stride:
            return None
        
        try:
            # å–å‡º VAD chunk
            vad_chunk = np.array(self.vad_buffer[:self.vad_chunk_stride], dtype=np.float32)
            
            # VAD æ£€æµ‹
            is_final = False
            try:
                with vad_model_lock:
                    vad_result = vad_model.generate(
                        input=vad_chunk,
                        cache=self.vad_cache,
                        is_final=is_final,
                        chunk_size=self.vad_chunk_size
                    )
            except Exception as e:
                print(f"âš ï¸ VAD æ£€æµ‹é”™è¯¯ [{self.session_id}]: {str(e)}")
                self.vad_cache = {}
                try:
                    with vad_model_lock:
                        vad_result = vad_model.generate(
                            input=vad_chunk,
                            cache=self.vad_cache,
                            is_final=is_final,
                            chunk_size=self.vad_chunk_size
                        )
                except Exception as e2:
                    print(f"âš ï¸ VAD é‡è¯•å¤±è´¥ [{self.session_id}]: {str(e2)}")
                    self.vad_buffer = self.vad_buffer[self.vad_chunk_stride:]
                    self.total_audio_ms += self.vad_chunk_size
                    return None

            self.vad_buffer = self.vad_buffer[self.vad_chunk_stride:]
            
            self.total_audio_ms += self.vad_chunk_size
            
            if vad_result and len(vad_result) > 0:
                segments = vad_result[0].get("value", [])
                
                # è§£æ VAD è¾“å‡º
                # [[beg, end]]: å®Œæ•´è¯­éŸ³æ®µ
                # [[beg, -1]]: åªæ£€æµ‹åˆ°èµ·å§‹ç‚¹
                # [[-1, end]]: åªæ£€æµ‹åˆ°ç»“æŸç‚¹
                # []: æ— æ£€æµ‹
                
                for seg in segments:
                    if len(seg) >= 2:
                        beg, end = seg[0], seg[1]
                        
                        if beg >= 0 and end == -1:
                            # æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
                            if not self.is_speech_active:
                                self.is_speech_active = True
                                self.speech_start_time = beg
                                return {'type': 'start', 'time': beg}
                        
                        elif beg == -1 and end >= 0:
                            # æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
                            if self.is_speech_active:
                                self.is_speech_active = False
                                return {'type': 'end', 'time': end}
                        
                        elif beg >= 0 and end >= 0:
                            # å®Œæ•´è¯­éŸ³æ®µï¼ˆå¼€å§‹å’Œç»“æŸï¼‰
                            return {'type': 'segment', 'start': beg, 'end': end}
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ VAD æ£€æµ‹é”™è¯¯ [{self.session_id}]: {str(e)}")
            return None
    
    def _apply_realtime_punc(self, text):
        """ä½¿ç”¨å®æ—¶æ ‡ç‚¹æ¨¡å‹æ·»åŠ æ ‡ç‚¹
        
        å®æ—¶æ ‡ç‚¹æ¨¡å‹æ”¯æŒæµå¼å¤„ç†ï¼Œä¼šæ ¹æ®ä¸Šä¸‹æ–‡æ™ºèƒ½æ·»åŠ æ ‡ç‚¹
        """
        if not text or not punc_realtime_model:
            return text
        
        try:
            with punc_model_lock:
                punc_result = punc_realtime_model.generate(
                    input=text,
                    cache=self.punc_cache
                )
            if punc_result and len(punc_result) > 0:
                return punc_result[0].get("text", text)
        except Exception as e:
            print(f"âš ï¸ å®æ—¶æ ‡ç‚¹æ¢å¤å¤±è´¥: {str(e)}")
        
        return text
        
    def process_audio(self):
        """å¤„ç†ç¼“å†²åŒºä¸­çš„éŸ³é¢‘ï¼ˆæµå¼ï¼‰
        
        ä¼˜åŒ–é€»è¾‘ï¼š
        1. å…ˆè¿›è¡Œ VAD æ£€æµ‹ï¼Œè·å–è¯­éŸ³ç«¯ç‚¹ä¿¡æ¯
        2. è¿›è¡Œæµå¼ ASR è¯†åˆ«
        3. ä½¿ç”¨å®æ—¶æ ‡ç‚¹æ¨¡å‹æ·»åŠ æ ‡ç‚¹
        4. å½“ VAD æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸæ—¶ï¼Œå¼ºåˆ¶è¾“å‡ºå½“å‰å¥å­
        """
        # å…ˆå¤„ç† VAD
        vad_event = self._process_vad()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ•°æ®è¿›è¡Œ ASRï¼ˆ600msï¼‰
        if len(self.audio_buffer) < self.asr_chunk_stride:
            # å¦‚æœæœ‰ VAD äº‹ä»¶ä½†æ²¡æœ‰è¶³å¤ŸéŸ³é¢‘ï¼Œè¿”å› VAD çŠ¶æ€
            if vad_event:
                return {
                    "text": "",
                    "punc_text": "",
                    "full_text": self.text_with_punc + self.pending_text,
                    "is_final": False,
                    "vad_event": vad_event
                }
            return None
        
        try:
            # å–å‡ºä¸€ä¸ª chunk çš„éŸ³é¢‘
            speech_chunk = np.array(self.audio_buffer[:self.asr_chunk_stride], dtype=np.float32)
            
            # æµå¼ ASR è¯†åˆ«
            try:
                with asr_model_lock:
                    asr_result = asr_model.generate(
                        input=speech_chunk,
                        cache=self.asr_cache,
                        is_final=False,
                        chunk_size=self.chunk_size,
                        encoder_chunk_look_back=4,
                        decoder_chunk_look_back=1,
                    )
            except Exception as e:
                print(f"âŒ æµå¼è¯†åˆ«é”™è¯¯ [{self.session_id}]: {str(e)}")
                self.asr_cache = {}
                try:
                    with asr_model_lock:
                        asr_result = asr_model.generate(
                            input=speech_chunk,
                            cache=self.asr_cache,
                            is_final=False,
                            chunk_size=self.chunk_size,
                            encoder_chunk_look_back=4,
                            decoder_chunk_look_back=1,
                        )
                except Exception as e2:
                    print(f"âŒ æµå¼è¯†åˆ«é‡è¯•å¤±è´¥ [{self.session_id}]: {str(e2)}")
                    self.audio_buffer = self.audio_buffer[self.asr_chunk_stride:]
                    self.asr_processed_ms += 600
                    return None

            self.audio_buffer = self.audio_buffer[self.asr_chunk_stride:]
            
            # è®°å½•å½“å‰ chunk çš„æ—¶é—´èŒƒå›´
            chunk_start_ms = self.asr_processed_ms
            chunk_end_ms = chunk_start_ms + 600  # æ¯ä¸ª chunk 600ms
            self.asr_processed_ms = chunk_end_ms
            
            text = ""
            punc_text = ""
            current_segment = None
            
            if asr_result and len(asr_result) > 0:
                text = asr_result[0].get("text", "")
                
                if text:
                    # ç´¯ç§¯åŸå§‹æ–‡æœ¬
                    self.all_text += text
                    self.pending_text += text
                    self.sentence_buffer += text
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ ‡ç‚¹å¤„ç†
                    # æ¡ä»¶ï¼šVAD æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œæˆ–ç´¯ç§¯æ–‡æœ¬è¶…è¿‡é˜ˆå€¼
                    should_apply_punc = False
                    
                    if vad_event and vad_event.get('type') == 'end':
                        # VAD æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œå¼ºåˆ¶å¤„ç†å½“å‰å¥å­
                        should_apply_punc = True
                    elif len(self.pending_text) >= 20:
                        # ç´¯ç§¯è¶…è¿‡ 20 å­—ç¬¦æ—¶å¤„ç†
                        should_apply_punc = True
                    
                    if should_apply_punc and self.pending_text:
                        # ä½¿ç”¨å®æ—¶æ ‡ç‚¹æ¨¡å‹
                        punc_text = self._apply_realtime_punc(self.pending_text)
                        self.text_with_punc += punc_text
                        
                        # è®°å½•å¸¦æ—¶é—´æˆ³çš„ç‰‡æ®µï¼ˆå®æ—¶ç²—ç•¥æ—¶é—´æˆ³ï¼‰
                        current_segment = {
                            'text': punc_text,
                            'start_ms': self.current_segment_start,
                            'end_ms': chunk_end_ms
                        }
                        self.segments.append(current_segment)
                        
                        # æ›´æ–°ä¸‹ä¸€ä¸ªç‰‡æ®µçš„èµ·å§‹æ—¶é—´
                        self.current_segment_start = chunk_end_ms
                        self.pending_text = ""
                        
                        # å¦‚æœæ˜¯ VAD ç»“æŸäº‹ä»¶ï¼Œé‡ç½®å¥å­ç¼“å†²åŒº
                        if vad_event and vad_event.get('type') == 'end':
                            self.sentence_buffer = ""
            
            return {
                "text": text,
                "punc_text": punc_text,
                "full_text": self.text_with_punc + self.pending_text,
                "is_final": False,
                "vad_event": vad_event,
                "is_speech_active": self.is_speech_active,
                "segment": current_segment,  # å½“å‰ç‰‡æ®µçš„æ—¶é—´æˆ³ä¿¡æ¯
                "current_time_ms": chunk_end_ms  # å½“å‰éŸ³é¢‘æ—¶é—´
            }
            
        except Exception as e:
            print(f"âŒ æµå¼è¯†åˆ«é”™è¯¯ [{self.session_id}]: {str(e)}")
            return None
    
    def finalize(self):
        """å®Œæˆè¯†åˆ«ï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ"""
        try:
            # å¤„ç†æœ€åå‰©ä½™çš„éŸ³é¢‘
            if len(self.audio_buffer) >= 4800:  # è‡³å°‘ 300ms
                speech_chunk = np.array(self.audio_buffer, dtype=np.float32)
                with asr_model_lock:
                    asr_result = asr_model.generate(
                        input=speech_chunk,
                        cache=self.asr_cache,
                        is_final=True,
                        chunk_size=self.chunk_size,
                    )
                
                if asr_result and len(asr_result) > 0:
                    text = asr_result[0].get("text", "")
                    if text:
                        self.all_text += text
                        self.pending_text += text
            
            # å¯¹å‰©ä½™å¾…å¤„ç†æ–‡æœ¬ä½¿ç”¨å®æ—¶æ ‡ç‚¹æ¨¡å‹
            if self.pending_text:
                punc_text = self._apply_realtime_punc(self.pending_text)
                self.text_with_punc += punc_text
            
            paraformer_text = self.text_with_punc
            print(f"âœ… Paraformerå®Œæ•´æ–‡æœ¬: {paraformer_text} ({len(paraformer_text)}å­—)")
            
            # ä½¿ç”¨ VADåˆ†æ®µ + SenseVoiceè¯†åˆ«ï¼ˆä¸å†è‡ªåŠ¨è°ƒç”¨LLMçº é”™ï¼‰
            sensevoice_text = ""
            timestamps = []
            if len(self.full_audio) > 0:
                print(f"ğŸ” å¼€å§‹VADåˆ†æ®µ+SenseVoiceè¯†åˆ«...")
                try:
                    # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                    audio_array = np.array(self.full_audio, dtype=np.float32)
                    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                    temp_path = temp_file.name
                    temp_file.close()
                    sf.write(temp_path, audio_array, self.sample_rate)
                    
                    # è°ƒç”¨ä¸å¸¦LLMçš„SenseVoiceè¯†åˆ«
                    sensevoice_text, timestamps = _run_sensevoice_with_timestamps(temp_path)
                    os.remove(temp_path)
                    
                    print(f"âœ… å®Œæˆ: SenseVoiceæ–‡æœ¬ {len(sensevoice_text)}å­—, {len(timestamps)} ä¸ªå¥å­")
                except Exception as e:
                    print(f"âŒ VAD+SenseVoiceè¯†åˆ«å¤±è´¥: {str(e)}")
                    # é™çº§ï¼šä½¿ç”¨æ™®é€šè¯†åˆ«
                    try:
                        sensevoice_text = _run_sensevoice_array(audio_array, self.sample_rate)
                    except:
                        pass
            
            return {
                'paraformer': paraformer_text,
                'sensevoice': sensevoice_text,
                'paraformer_length': len(paraformer_text),
                'sensevoice_length': len(sensevoice_text),
                'timestamps': timestamps,  # VADå¥çº§æ—¶é—´æˆ³ï¼ˆSenseVoiceåŸå§‹æ–‡æœ¬ï¼‰
                'realtime_segments': self.segments,  # å®æ—¶ç²—ç•¥æ—¶é—´æˆ³ï¼ˆå¤‡ç”¨ï¼‰
            }
            
        except Exception as e:
            print(f"âŒ æœ€ç»ˆè¯†åˆ«é”™è¯¯: {str(e)}")
            return {
                'paraformer': self.text_with_punc + self.pending_text,
                'sensevoice': '',
                'paraformer_length': len(self.text_with_punc + self.pending_text),
                'sensevoice_length': 0,
                'llm_merged_length': 0,
            }


# ==================== WebSocket äº‹ä»¶å¤„ç† ====================

@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥"""
    session_id = request.sid
    print(f"âœ… å®¢æˆ·ç«¯è¿æ¥: {session_id}")
    emit('connected', {'session_id': session_id})


@socketio.on('disconnect')
def handle_disconnect():
    """å®¢æˆ·ç«¯æ–­å¼€"""
    session_id = request.sid
    with active_sessions_lock:
        active_sessions.pop(session_id, None)
    print(f"âŒ å®¢æˆ·ç«¯æ–­å¼€: {session_id}")


@socketio.on('start_recording')
def handle_start_recording():
    """å¼€å§‹å½•éŸ³"""
    session_id = request.sid
    with active_sessions_lock:
        active_sessions[session_id] = RealtimeASR(session_id)
    print(f"ğŸ™ï¸ å¼€å§‹å½•éŸ³: {session_id}")
    emit('recording_started', {'status': 'ok'})


@socketio.on('audio_data')
def handle_audio_data(data):
    """æ¥æ”¶éŸ³é¢‘æ•°æ®"""
    session_id = request.sid

    with active_sessions_lock:
        asr = active_sessions.get(session_id)
 
    if not asr:
        emit('error', {'message': 'ä¼šè¯ä¸å­˜åœ¨'})
        return
 
    if getattr(asr, 'is_finalizing', False):
        return
 
    try:
        with asr.lock:
            if asr.is_finalizing:
                return
            asr.add_audio(data)
 
            # å¤„ç†éŸ³é¢‘å¹¶è¿”å›å®æ—¶ç»“æœ
            result = asr.process_audio()
            if result:
                emit('transcription', result)
    except Exception as e:
        print(f"âŒ éŸ³é¢‘å¤„ç†é”™è¯¯ [{session_id}]: {str(e)}")
        # ä¸å‘é€é”™è¯¯ï¼Œé¿å…ä¸­æ–­å½•éŸ³æµç¨‹


@socketio.on('stop_recording')
def handle_stop_recording():
    """åœæ­¢å½•éŸ³"""
    session_id = request.sid

    with active_sessions_lock:
        asr = active_sessions.get(session_id)
 
    if not asr:
        emit('error', {'message': 'ä¼šè¯ä¸å­˜åœ¨'})
        return
 
    asr.is_finalizing = True
    print(f"ğŸ›‘ åœæ­¢å½•éŸ³: {session_id}")
     
    # é€šçŸ¥å‰ç«¯å½•éŸ³å·²åœæ­¢ï¼Œå¼€å§‹LLMå¤„ç†
    emit('recording_stopped', {'message': 'å½•éŸ³å·²åœæ­¢ï¼Œå¼€å§‹LLMçº é”™'})
     
    try:
        # ç”Ÿæˆæœ€ç»ˆç»“æœï¼ˆå¯èƒ½è€—æ—¶è¾ƒé•¿ï¼ŒåŒ…å«SenseVoiceå’ŒLLMå¤„ç†ï¼‰
        with asr.lock:
            final_result = asr.finalize()
        emit('final_result', final_result)
    except Exception as e:
        print(f"âŒ æœ€ç»ˆå¤„ç†é”™è¯¯ [{session_id}]: {str(e)}")
        traceback.print_exc()
        # è¿”å›å·²æœ‰çš„éƒ¨åˆ†ç»“æœ
        emit('final_result', {
            'paraformer': asr.text_with_punc + asr.pending_text,
            'sensevoice': '',
            'llm_merged': '',
            'paraformer_length': len(asr.text_with_punc + asr.pending_text),
            'sensevoice_length': 0,
            'llm_merged_length': 0,
            'error': str(e)
         })
    finally:
        # ç¡®ä¿æ¸…ç†ä¼šè¯
        with active_sessions_lock:
            active_sessions.pop(session_id, None)


# ==================== REST API è·¯ç”± ====================

@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    return jsonify({
        "status": "ok",
        "message": "ASR APIæœåŠ¡æ­£å¸¸è¿è¡Œ",
        "models_loaded": asr_model is not None
    }), 200


@app.route('/api/asr/transcribe', methods=['POST'])
def transcribe_audio():
    """
    éŸ³é¢‘æ–‡ä»¶è½¬å½•æ¥å£
    ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨SenseVoiceè¿›è¡Œè¯†åˆ«ï¼Œå¹¶ç”Ÿæˆç²¾ç¡®æ—¶é—´æˆ³
    æ”¯æŒå‚æ•°ï¼š
    - file: éŸ³é¢‘æ–‡ä»¶
    - generate_timestamps: æ˜¯å¦ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆé»˜è®¤trueï¼‰
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "æœªæ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ 'file' å­—æ®µä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
            }), 400
        
        file = request.files['file']
        generate_ts = request.form.get('generate_timestamps', 'true').lower() == 'true'
        
        # æ£€æŸ¥æ–‡ä»¶å
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "æ–‡ä»¶åä¸ºç©º"
            }), 400
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if not allowed_file(file.filename):
            return jsonify({
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(ALLOWED_EXTENSIONS)}"
            }), 400
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        temp_upload = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1])
        temp_upload_path = temp_upload.name
        file.save(temp_upload_path)
        temp_upload.close()
        
        # å°†éŸ³é¢‘è½¬æ¢ä¸ºWAVæ ¼å¼ï¼ˆç¡®ä¿æ‰€æœ‰æ ¼å¼éƒ½èƒ½è¢«æ­£ç¡®å¤„ç†ï¼‰
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        temp_path = temp_wav.name
        temp_wav.close()
        
        try:
            print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
            
            # ä½¿ç”¨librosaè¯»å–å¹¶è½¬æ¢ä¸ºWAVæ ¼å¼
            print("ğŸ”„ è½¬æ¢éŸ³é¢‘æ ¼å¼...")
            audio_data, sr = librosa.load(temp_upload_path, sr=16000, mono=True)
            sf.write(temp_path, audio_data, sr)
            print(f"âœ… æ ¼å¼è½¬æ¢å®Œæˆ: 16kHz, å•å£°é“")
            
            # è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
            audio_duration_ms = int(len(audio_data) / sr * 1000)
            
            # åˆ é™¤ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_upload_path)
            
            # ä½¿ç”¨SenseVoiceè¯†åˆ«ï¼ˆå¸¦VADå¥çº§æ—¶é—´æˆ³ï¼‰
            print("âœ¨ SenseVoiceè¯†åˆ«ä¸­...")
            if generate_ts:
                sensevoice_text, timestamps = _run_sensevoice_with_timestamps(temp_path)
                print(f"âœ… SenseVoiceå®Œæˆ: {len(sensevoice_text)}å­—, {len(timestamps)} ä¸ªå¥å­")
            else:
                sensevoice_text = _run_sensevoice(temp_path)
                timestamps = []
                print(f"âœ… SenseVoiceå®Œæˆ: {len(sensevoice_text)}å­—")
            
            # è¿”å›å®Œæ•´ç»“æœ
            return jsonify({
                "success": True,
                "data": {
                    "text": sensevoice_text,
                    "length": len(sensevoice_text),
                    "model": "SenseVoice",
                    "timestamps": timestamps,
                    "duration_ms": audio_duration_ms
                },
                "filename": file.filename,
                "mode": "file_upload"
            }), 200
            
        finally:
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
    except Exception as e:
        print(f"âŒ å¤„ç†é”™è¯¯: {str(e)}")
        traceback.print_exc()
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/api/asr/models', methods=['GET'])
def get_models_info():
    """
    è·å–æ¨¡å‹ä¿¡æ¯
    """
    return jsonify({
        "success": True,
        "data": {
            "asr_model": "paraformer-zh-streaming",
            "punc_model": "ct-punc",
            "sensevoice_model": "iic/SenseVoiceSmall",
            "llm_model": LLM_MODEL,
            "models_loaded": asr_model is not None
        }
    }), 200


@app.route('/api/asr/formats', methods=['GET'])
def get_supported_formats():
    """
    è·å–æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
    """
    return jsonify({
        "success": True,
        "data": {
            "formats": list(ALLOWED_EXTENSIONS),
            "description": "æ”¯æŒçš„éŸ³é¢‘æ–‡ä»¶æ ¼å¼"
        }
    }), 200


if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ“£ è¯­éŸ³è¯†åˆ«APIæœåŠ¡å™¨")
    print("=" * 60)
    print("ğŸ“ æ”¯æŒæ¨¡å¼:")
    print("  1. å®æ—¶å½•éŸ³æ¨¡å¼ï¼ˆWebSocketï¼‰:")
    print("     - Paraformer å®æ—¶æµå¼è¯†åˆ«")
    print("     - SenseVoice å®Œæ•´éŸ³é¢‘è¯†åˆ«ï¼ˆå¸¦VADæ—¶é—´æˆ³ï¼‰")
    print("  2. æ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼ˆREST APIï¼‰:")
    print("     - SenseVoice è¯†åˆ«ï¼ˆå¸¦VADæ—¶é—´æˆ³ï¼‰")
    print("=" * 60)
    print("ğŸ”§ REST APIæ¥å£:")
    print("  - GET  /api/health              å¥åº·æ£€æŸ¥")
    print("  - POST /api/asr/transcribe      æ–‡ä»¶è½¬å½•ï¼ˆSenseVoice+VADæ—¶é—´æˆ³ï¼‰")
    print("  - GET  /api/asr/models          æ¨¡å‹ä¿¡æ¯")
    print("  - GET  /api/asr/formats         æ”¯æŒæ ¼å¼")
    print("")
    print("ğŸ”Œ WebSocketæ¥å£:")
    print("  - connect                    å»ºç«‹è¿æ¥")
    print("  - start_recording            å¼€å§‹å½•éŸ³")
    print("  - audio_data                 å‘é€éŸ³é¢‘æ•°æ®")
    print("  - stop_recording             åœæ­¢å½•éŸ³")
    print("  - transcription              æ¥æ”¶å®æ—¶è¯†åˆ«")
    print("  - recording_stopped          å½•éŸ³å·²åœæ­¢ï¼Œå¼€å§‹LLMå¤„ç†")
    print("  - final_result               æ¥æ”¶æœ€ç»ˆç»“æœ")
    print("=" * 60)
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5006")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹
    init_models()
    
    # å¯åŠ¨æœåŠ¡ï¼ˆä½¿ç”¨socketio.runæ”¯æŒWebSocketï¼‰
    socketio.run(app, host='0.0.0.0', port=5006, debug=False, allow_unsafe_werkzeug=True)
